>>> df1 = spark.read.format("csv").option("header","true").option("delimiter","|").load("file:///home/gopalkrishna/Abhi/empdata.log")
>>> 
>>> df1
DataFrame[eno: string, ecomp: string, esal: string, ecity: string]
>>> 
>>> 
>>> df1.show(2)
+---+-----------------+------+---------+
|eno|            ecomp|  esal|    ecity|
+---+-----------------+------+---------+
|100|UnitedHealthGroup|  null|Hyderabad|
|200|   InfosysTechLtd|700000|Bangalore|
+---+-----------------+------+---------+
only showing top 2 rows

>>> from_mysql = sc.parallelize([(1, 'Rahul'), (2, 'Shyam'), (3, 'Shyam1'), (4, 'Rahul1'), (5, 'rohan'), (6, 'omkar'), (7, 'somnath'), (8, 'vipul'), (9, 'vicky')])
>>> 
>>> 
>>> from_mysql
ParallelCollectionRDD[82] at parallelize at PythonRDD.scala:475
>>> 
>>> from pyspark.sql.types import *
>>> schema = StructType([StructField("eno",IntegerType(), True), StructField("ename",StringType(), True)])
>>> 
>>> df2 = spark.createDataFrame(from_mysql,schema)
>>> 
>>> df2.show()
+---+-------+
|eno|  ename|
+---+-------+
|  1|  Rahul|
|  2|  Shyam|
|  3| Shyam1|
|  4| Rahul1|
|  5|  rohan|
|  6|  omkar|
|  7|somnath|
|  8|  vipul|
|  9|  vicky|
+---+-------+

>>> df2.printSchema
<bound method DataFrame.printSchema of DataFrame[eno: int, ename: string]>
>>> 
>>> joinedDF = df1.join(broadcast(df2), df1.eno == df2.eno).drop(df2.eno)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'broadcast' is not defined
>>> 
>>> from pyspark.sql.functions import broadcast
>>> joinedDF = df1.join(broadcast(df2), df1.eno == df2.eno).drop(df2.eno)
>>> 
>>> >>> 
>>> 
>>> 
>>> from_mysql = sc.parallelize([(1, 'Rahul'), (2, 'Shyam'), (3, 'Shyam1'), (4, 'Rahul1'), (5, 'rohan'), (6, 'omkar'), (7, 'somnath'), (8, 'vipul'), (9, 'vicky'), (100, 'Shubham'), (200, 'Nilesh'), (400, 'Ankush')])
>>> 
>>> df2 = spark.createDataFrame(from_mysql,schema)>>> 
>>> 
>>> df2.show()
+---+-------+
|eno|  ename|
+---+-------+
|  1|  Rahul|
|  2|  Shyam|
|  3| Shyam1|
|  4| Rahul1|
|  5|  rohan|
|  6|  omkar|
|  7|somnath|
|  8|  vipul|
|  9|  vicky|
|100|Shubham|
|200| Nilesh|
|400| Ankush|
+---+-------+

>>> 
>>> joinedDF = df1.join(broadcast(df2), df1.eno == df2.eno).drop(df2.eno)
>>> joinedDF.show()
+---+--------------------+------+---------+-------+
|eno|               ecomp|  esal|    ecity|  ename|
+---+--------------------+------+---------+-------+
|100|   UnitedHealthGroup|  null|Hyderabad|Shubham|
|200|      InfosysTechLtd|700000|Bangalore| Nilesh|
|400|TataConsultancySe...|320000|     null| Ankush|
+---+--------------------+------+---------+-------+

>>> 
>>> df3 = joinedDF.withColumn("hired_date",current_date())
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'current_date' is not defined
>>> 
>>> from pyspark.sql.functions import *
>>> 
>>> df3 = joinedDF.withColumn("hired_date",current_date())
>>> 
>>> df3.show()
+---+--------------------+------+---------+-------+----------+
|eno|               ecomp|  esal|    ecity|  ename|hired_date|
+---+--------------------+------+---------+-------+----------+
|100|   UnitedHealthGroup|  null|Hyderabad|Shubham|2022-11-23|
|200|      InfosysTechLtd|700000|Bangalore| Nilesh|2022-11-23|
|400|TataConsultancySe...|320000|     null| Ankush|2022-11-23|
+---+--------------------+------+---------+-------+----------+

>>> 
>>> res_df = df3.withColumn("emp_state",when(upper(trim(df3.ecity)) == "HYDERABAD","TS").when(upper(trim(df3.ecity)) == "BANGALORE", "KS").otherwise("MH"))
>>> 
>>> res_df.show()
+---+--------------------+------+---------+-------+----------+---------+
|eno|               ecomp|  esal|    ecity|  ename|hired_date|emp_state|
+---+--------------------+------+---------+-------+----------+---------+
|100|   UnitedHealthGroup|  null|Hyderabad|Shubham|2022-11-23|       TS|
|200|      InfosysTechLtd|700000|Bangalore| Nilesh|2022-11-23|       KS|
|400|TataConsultancySe...|320000|     null| Ankush|2022-11-23|       MH|
+---+--------------------+------+---------+-------+----------+---------+

>>> 
>>> res_df.write.mode("overwrite").parquet("hdfs://localhost:8020/curated_data")

>>> 
>>> res_df.write.mode("overwrite").parquet("hdfs://localhost:8020/curated_data")
>>> 
>>> parquetDF = spark.read.parquet("hdfs://localhost:8020/curated_data")
>>> parquetDF.show()
+---+--------------------+------+---------+-------+----------+---------+
|eno|               ecomp|  esal|    ecity|  ename|hired_date|emp_state|
+---+--------------------+------+---------+-------+----------+---------+
|100|   UnitedHealthGroup|  null|Hyderabad|Shubham|2022-11-23|       TS|
|200|      InfosysTechLtd|700000|Bangalore| Nilesh|2022-11-23|       KS|
|400|TataConsultancySe...|320000|     null| Ankush|2022-11-23|       MH|
+---+--------------------+------+---------+-------+----------+---------+

>>> 
>>> master_df = parquetDF.na.fill("NA")
>>> master_df.show()
+---+--------------------+------+---------+-------+----------+---------+
|eno|               ecomp|  esal|    ecity|  ename|hired_date|emp_state|
+---+--------------------+------+---------+-------+----------+---------+
|100|   UnitedHealthGroup|    NA|Hyderabad|Shubham|2022-11-23|       TS|
|200|      InfosysTechLtd|700000|Bangalore| Nilesh|2022-11-23|       KS|
|400|TataConsultancySe...|320000|       NA| Ankush|2022-11-23|       MH|
+---+--------------------+------+---------+-------+----------+---------+

>>> 
>>> master_df.write.mode("overwrite").partitionBy("ecity").csv("hdfs://localhost:8020/master_Data")
>>> 
>>> 
>>> 


